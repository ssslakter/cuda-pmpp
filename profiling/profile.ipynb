{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch._logging.set_logs(output_code=True)\n",
    "f = torch.compile(torch.softmax)\n",
    "x = torch.randn(10).cuda()\n",
    "f(x, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_pytorch_function(func, input):\n",
    "    # CUDA IS ASYNC so can't use python time module\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    # Warmup\n",
    "    for _ in range(5): func(input)\n",
    "\n",
    "    start.record()\n",
    "    func(input)\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    return start.elapsed_time(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_1(x): return x*x\n",
    "def square_2(x): return x**2\n",
    "b = torch.randn(10000, 10000).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-04-07 15:58:27 62509:62509 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "            ProfilerStep*         0.16%       1.880ms        99.76%        1.162s     581.157ms             2  \n",
      "              aten::randn         0.00%      51.000us        86.25%        1.005s     502.421ms             2  \n",
      "              aten::empty         0.00%      50.000us         0.00%      50.000us      25.000us             2  \n",
      "            aten::normal_        86.24%        1.005s        86.24%        1.005s     502.371ms             2  \n",
      "                 aten::to         0.01%      65.000us        13.33%     155.310ms      38.828ms             4  \n",
      "           aten::_to_copy         0.00%      53.000us        13.32%     155.245ms      77.623ms             2  \n",
      "      aten::empty_strided         0.00%      54.000us         0.00%      54.000us      27.000us             2  \n",
      "              aten::copy_         0.01%      93.000us        13.32%     155.138ms      77.569ms             2  \n",
      "          cudaMemcpyAsync        13.28%     154.708ms        13.28%     154.708ms      77.354ms             2  \n",
      "    cudaStreamSynchronize         0.03%     337.000us         0.03%     337.000us     168.500us             2  \n",
      "             aten::square         0.00%      13.000us         0.02%     281.000us     140.500us             2  \n",
      "                aten::pow         0.01%     153.000us         0.02%     268.000us     134.000us             2  \n",
      "        aten::result_type         0.00%       2.000us         0.00%       2.000us       1.000us             2  \n",
      "         cudaLaunchKernel         0.01%     113.000us         0.01%     113.000us      56.500us             2  \n",
      "    cudaDeviceSynchronize         0.24%       2.752ms         0.24%       2.752ms       2.752ms             1  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.165s\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-04-07 15:58:28 62509:62509 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-04-07 15:58:28 62509:62509 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "def trace_handler(prof):\n",
    "    print(prof.key_averages().table(\n",
    "        sort_by=\"self_cuda_time_total\", row_limit=-1))\n",
    "    prof.export_chrome_trace(\"/tmp/test_trace_\" + str(prof.step_num) + \".json\")\n",
    "\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ],\n",
    "\n",
    "    # In this example with wait=1, warmup=1, active=2, repeat=1,\n",
    "    # profiler will skip the first step/iteration,\n",
    "    # start warming up on the second, record\n",
    "    # the third and the forth iterations,\n",
    "    # after which the trace will become available\n",
    "    # and on_trace_ready (when set) is called;\n",
    "    # the cycle repeats starting with the next step\n",
    "\n",
    "    schedule=torch.profiler.schedule(\n",
    "        wait=1,\n",
    "        warmup=1,\n",
    "        active=2,\n",
    "        repeat=1),\n",
    "    on_trace_ready=trace_handler\n",
    "    # on_trace_ready=torch.profiler.tensorboard_trace_handler('./log')\n",
    "    # used when outputting for tensorboard\n",
    "    ) as p:\n",
    "        for iter in range(10):\n",
    "            torch.square(torch.randn(10000, 10000).cuda())\n",
    "            # send a signal to the profiler that the next iteration has started\n",
    "            p.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
