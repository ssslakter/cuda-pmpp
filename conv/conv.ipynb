{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from torchvision.io import read_image, write_png\n",
    "from profiling.profiler import profile\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "from numba.cuda import as_cuda_array as ca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"500\" src=\"../images/image.png\" id=\"jupyter\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_size = 3\n",
    "conv = torch.nn.Conv2d(1, 1, k_size, bias=False, padding=k_size//2).cuda()\n",
    "m1 = torch.rand(1000, 2000).contiguous().cuda()\n",
    "f = conv.weight[0][0].detach().contiguous().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic convolution kernel (without shared memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start from numba for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def conv2d_k(m, f, out, r):\n",
    "    # get row and column indices\n",
    "    row,col = cuda.grid(2)\n",
    "    if row < out.shape[0] and col < out.shape[1]:  # Ensure threads are within output shape\n",
    "        val = 0\n",
    "        for i in range(f.shape[0]):\n",
    "            for j in range(f.shape[1]):\n",
    "                in_row = row - r + i\n",
    "                in_col = col - r +j\n",
    "                if (m.shape[0]>in_row >=0 and m.shape[1]>in_col >=0):\n",
    "                    val += m[in_row, in_col] * f[i, j]  # Convolution operation\n",
    "        out[row, col] = val  # Store result in output array\n",
    "\n",
    "\n",
    "def conv_2d(m, f):\n",
    "    h,w  = m.shape\n",
    "    out = torch.zeros(h, w, dtype=m.dtype, device=m.device)\n",
    "    # TOTAL block size is limited by 1024 threads\n",
    "    block_size = 32\n",
    "    blocks = cdiv(h,block_size), cdiv(w,block_size)\n",
    "    conv2d_k[blocks, (block_size, block_size)](ca(m), ca(f), ca(out), f.shape[0]//2) \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isclose(conv(m1[None,]), conv_2d(m1,f), atol=1e-7).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit conv_2d(m1,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113 µs ± 3.07 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit with torch.no_grad(): conv(m1[None,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now rewrite into CUDA kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = load_cu_file('./conv2d.cu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isclose(conv(m1[None,]), mod.conv2d(m1,f)).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we're slightly slower than pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154 µs ± 254 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit mod.conv2d(m1,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With shared memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to use tiled convolution where we collaboratively store patches of matrix into shared memory and then reuse it later when computing convolution. Another way is to also load padding to cover whole edges but there are already high chances to hit L2 cache for big matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isclose(conv(m1[None,]), mod.conv2d_shared(m1,f)).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unexpectedly tiled convolution works slower than a naive one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154 µs ± 463 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit mod.conv2d(m1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185 µs ± 909 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit mod.conv2d_shared(m1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "            ProfilerStep*        46.95%     292.000us        67.68%     421.000us     210.500us             2  \n",
      "              aten::zeros         2.89%      18.000us        18.97%     118.000us      59.000us             2  \n",
      "              aten::empty         4.66%      29.000us         4.66%      29.000us      14.500us             2  \n",
      "              aten::zero_         1.61%      10.000us        11.41%      71.000us      35.500us             2  \n",
      "              aten::fill_         3.54%      22.000us         9.81%      61.000us      30.500us             2  \n",
      "         cudaLaunchKernel         8.04%      50.000us         8.04%      50.000us      12.500us             4  \n",
      "    cudaDeviceSynchronize        32.32%     201.000us        32.32%     201.000us     201.000us             1  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 622.000us\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-04-15 23:31:10 38918:38918 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-04-15 23:31:10 38918:38918 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-04-15 23:31:10 38918:38918 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "profile(partial(mod.conv2d,m1[None,]), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_1d = load_cu_file('./conv1d.cu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_size = 3\n",
    "conv = torch.nn.Conv1d(1, 1, k_size, bias=False, padding=k_size//2).cuda()\n",
    "m1 = torch.rand(2000).contiguous().cuda()\n",
    "f = conv.weight[0][0].detach().contiguous().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.isclose(conv(m1[None,]), mod_1d.conv1d(m1,f)).all()\n",
    "assert torch.isclose(conv(m1[None,]), mod_1d.conv1d_shared(m1,f)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
