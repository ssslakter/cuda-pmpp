{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from torchvision.io import read_image, write_png\n",
    "from torch.utils.cpp_extension import load_inline\n",
    "from profiling.profiler import profile\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "from numba.cuda import as_cuda_array as ca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"500\" src=\"../images/image.png\" id=\"jupyter\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_size = 3\n",
    "conv = torch.nn.Conv2d(1, 1, k_size, bias=False, padding=k_size//2).cuda()\n",
    "m1 = torch.rand(1000, 2000).contiguous().cuda()\n",
    "f = conv.weight[0][0].detach().contiguous().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic convolution kernel (without shared memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start from numba for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def conv2d_k(m, f, out, r):\n",
    "    # get row and column indices\n",
    "    row,col = cuda.grid(2)\n",
    "    if row < out.shape[0] and col < out.shape[1]:  # Ensure threads are within output shape\n",
    "        val = 0\n",
    "        for i in range(f.shape[0]):\n",
    "            for j in range(f.shape[1]):\n",
    "                in_row = row - r + i\n",
    "                in_col = col - r +j\n",
    "                if (m.shape[0]>in_row >=0 and m.shape[1]>in_col >=0):\n",
    "                    val += m[in_row, in_col] * f[i, j]  # Convolution operation\n",
    "        out[row, col] = val  # Store result in output array\n",
    "\n",
    "\n",
    "def conv_2d(m, f):\n",
    "    h,w  = m.shape\n",
    "    out = torch.zeros(h, w, dtype=m.dtype, device=m.device)\n",
    "    # TOTAL block size is limited by 1024 threads\n",
    "    block_size = 32\n",
    "    blocks = cdiv(h,block_size), cdiv(w,block_size)\n",
    "    conv2d_k[blocks, (block_size, block_size)](ca(m), ca(f), ca(out), f.shape[0]//2) \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isclose(conv(m1[None,]), conv_2d(m1,f), atol=1e-7).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit conv_2d(m1,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113 µs ± 123 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit with torch.no_grad(): conv(m1[None,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now rewrite into CUDA kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_src = cuda_begin + r'''\n",
    "__global__ void conv2d_k(float* m, float* f, float* out, int f_size, int m_h, int m_w) {\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int r = f_size/2;\n",
    "\n",
    "    if (row < m_h && col < m_w) {\n",
    "        float val = 0;\n",
    "        for (int i = 0; i < 2*r+1; i++) {\n",
    "            for (int j = 0; j < 2*r+1; j++) {\n",
    "                int in_row = row - r + i;\n",
    "                int in_col = col - r + j;\n",
    "                if (in_row >= 0 && in_row < m_h && in_col >= 0 && in_col < m_w) {\n",
    "                    val += m[in_row * m_w + in_col] * f[i*f_size+j];\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        out[row * m_w + col] = val;\n",
    "    }\n",
    "}\n",
    "\n",
    "torch::Tensor conv2d(torch::Tensor m, torch::Tensor f) {\n",
    "    CHECK_INPUT(m); CHECK_INPUT(f);\n",
    "    int h = m.size(0);\n",
    "    int w = m.size(1);\n",
    "    auto output = torch::zeros({h, w}, m.options());\n",
    "\n",
    "    dim3 tpb(16,16);\n",
    "    dim3 blocks(cdiv(w, tpb.x), cdiv(h, tpb.y));\n",
    "    conv2d_k<<<blocks, tpb>>>(\n",
    "        m.data_ptr<float>(), f.data_ptr<float>(), output.data_ptr<float>(), f.size(0), h, w);\n",
    "    C10_CUDA_KERNEL_LAUNCH_CHECK();\n",
    "    return output;\n",
    "}\n",
    "'''\n",
    "fname = 'conv2d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = load_cuda(cuda_src, get_sig(fname, cuda_src), [fname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isclose(conv(m1[None,]), mod.conv2d(m1,f)).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we're slightly slower than pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156 µs ± 3 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit mod.conv2d(m1,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With shared memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to use tiled convolution where we collaboratively store patches of matrix into shared memory and then reuse it later when computing convolution. Another way is to also load padding to cover whole edges but there are already high chances to hit L2 cache for big matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_src = cuda_begin + r'''\n",
    "#define TILE_DIM 16\n",
    "#define FILTER_RADIUS 1\n",
    "__constant__ float F_c[2*FILTER_RADIUS+1][2*FILTER_RADIUS+1];\n",
    "__global__ void conv2d_k(float* m, float* out, int m_h, int m_w) {\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int r = FILTER_RADIUS;\n",
    "    \n",
    "    __shared__ float tile[TILE_DIM][TILE_DIM];\n",
    "    if (row < m_h && col < m_w) {\n",
    "        tile[threadIdx.y][threadIdx.x] = m[row * m_w + col];\n",
    "    }\n",
    "    else{\n",
    "        tile[threadIdx.y][threadIdx.x] = 0.;\n",
    "    }\n",
    "    __syncthreads();\n",
    "    if (row < m_h && col < m_w) {\n",
    "        float val = 0;\n",
    "        for (int i = 0; i < 2*r+1; i++) {\n",
    "            for (int j = 0; j < 2*r+1; j++) {\n",
    "                if(threadIdx.x-r+j>=0 && \n",
    "                   threadIdx.x-r+j<TILE_DIM && \n",
    "                   threadIdx.y-r+i>=0 && \n",
    "                   threadIdx.y-r+i<TILE_DIM){\n",
    "                    val += tile[threadIdx.y+i-r][threadIdx.x+j-r] * F_c[i][j];\n",
    "                }\n",
    "                else{\n",
    "                    int in_row = row - r + i;\n",
    "                    int in_col = col - r + j;\n",
    "                    if (in_row >= 0 && in_row < m_h && in_col >= 0 && in_col < m_w) {\n",
    "                    val += m[in_row * m_w + in_col] * F_c[i][j];\n",
    "                }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        out[row * m_w + col] = val;\n",
    "    }\n",
    "}\n",
    "\n",
    "torch::Tensor conv2d_shared(torch::Tensor m, torch::Tensor f) {\n",
    "    CHECK_INPUT(m); CHECK_INPUT(f);\n",
    "    TORCH_CHECK(f.size(0)==2*FILTER_RADIUS+1 && f.size(1)==2*FILTER_RADIUS+1, \n",
    "    \"Filter size must be 2*FILTER_RADIUS+1 x 2*FILTER_RADIUS+1\");\n",
    "    int h = m.size(0);\n",
    "    int w = m.size(1);\n",
    "    auto output = torch::zeros({h, w}, m.options());\n",
    "    cudaMemcpyToSymbol(F_c, f.data_ptr<float>(), (2*FILTER_RADIUS+1)*(2*FILTER_RADIUS+1)*sizeof(float));\n",
    "\n",
    "    dim3 tpb(TILE_DIM,TILE_DIM);\n",
    "    dim3 blocks(cdiv(w, tpb.x), cdiv(h, tpb.y));\n",
    "    conv2d_k<<<blocks, tpb>>>(\n",
    "        m.data_ptr<float>(), output.data_ptr<float>(), h, w);\n",
    "    CUDA_ERR(cudaGetLastError());\n",
    "    C10_CUDA_KERNEL_LAUNCH_CHECK();\n",
    "    return output;\n",
    "}\n",
    "'''\n",
    "fname = 'conv2d_shared'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_s = load_cuda(cuda_src, get_sig(fname, cuda_src), [fname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isclose(conv(m1[None,]), mod_s.conv2d_shared(m1,f)).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unexpectedly tiled convolution works slower than a naive one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158 µs ± 243 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit mod.conv2d(m1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183 µs ± 783 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit mod_s.conv2d_shared(m1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "            ProfilerStep*        46.95%     292.000us        67.68%     421.000us     210.500us             2  \n",
      "              aten::zeros         2.89%      18.000us        18.97%     118.000us      59.000us             2  \n",
      "              aten::empty         4.66%      29.000us         4.66%      29.000us      14.500us             2  \n",
      "              aten::zero_         1.61%      10.000us        11.41%      71.000us      35.500us             2  \n",
      "              aten::fill_         3.54%      22.000us         9.81%      61.000us      30.500us             2  \n",
      "         cudaLaunchKernel         8.04%      50.000us         8.04%      50.000us      12.500us             4  \n",
      "    cudaDeviceSynchronize        32.32%     201.000us        32.32%     201.000us     201.000us             1  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 622.000us\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-04-15 23:31:10 38918:38918 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-04-15 23:31:10 38918:38918 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-04-15 23:31:10 38918:38918 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "profile(partial(mod.conv2d,m1[None,]), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
