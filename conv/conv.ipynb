{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from torchvision.io import read_image, write_png\n",
    "from torch.utils.cpp_extension import load_inline\n",
    "from utils import cdiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "from numba.cuda import as_cuda_array as ca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"500\" src=\"../images/image.png\" id=\"jupyter\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic convolution kernel (without shared memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def conv2d_k(m, f, out, r):\n",
    "    # get row and column indices\n",
    "    row,col = cuda.grid(2)\n",
    "    if row < out.shape[0] and col < out.shape[1]:  # Ensure threads are within output shape\n",
    "        val = 0\n",
    "        for i in range(f.shape[0]):\n",
    "            for j in range(f.shape[1]):\n",
    "                in_row = row - r + i\n",
    "                in_col = col - r +j\n",
    "                if (m.shape[0]>in_row >=0 and m.shape[1]>in_col >=0):\n",
    "                    val += m[in_row, in_col] * f[i, j]  # Convolution operation\n",
    "        out[row, col] = val  # Store result in output array\n",
    "\n",
    "\n",
    "def conv_2d(m, f):\n",
    "    h,w  = m.shape\n",
    "    out = torch.zeros(h, w, dtype=m.dtype, device=m.device)\n",
    "    # TOTAL block size is limited by 1024 threads\n",
    "    block_size = 32\n",
    "    blocks = cdiv(h,block_size), cdiv(w,block_size)\n",
    "    conv2d_k[blocks, (block_size, block_size)](ca(m), ca(f), ca(out), f.shape[0]//2) \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = torch.nn.Conv2d(1, 1, 3, bias=False, padding=1).cuda()\n",
    "m1 = torch.rand(1000, 2000).contiguous().cuda()\n",
    "f = conv.weight[0][0].detach().contiguous().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isclose(conv(m1[None,]), conv_2d(m1,f), atol=1e-7).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "487 µs ± 183 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit conv_2d(m1,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114 µs ± 413 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit with torch.no_grad(): conv(m1[None,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With constant memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = '''\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void conv2d_k(float *m, float *f, float *out, int r, int m_h, int m_w, int f_h, int f_w, int out_h, int out_w) {\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (row < out_h && col < out_w) {\n",
    "        float val = 0;\n",
    "        for (int i = 0; i < f_h; ++i) {\n",
    "            for (int j = 0; j < f_w; ++j) {\n",
    "                int in_row = row - r + i;\n",
    "                int in_col = col - r + j;\n",
    "                if (in_row >= 0 && in_row < m_h && in_col >= 0 && in_col < m_w) {\n",
    "                    val += m[in_row * m_w + in_col] * f[i * f_w + j];\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        out[row * out_w + col] = val;\n",
    "    }\n",
    "}\n",
    "\n",
    "void conv_2d(float *m, float *f, float *out, int m_h, int m_w, int f_h, int f_w, int out_h, int out_w) {\n",
    "    // Define block and grid dimensions\n",
    "    dim3 blockDim(32, 32);\n",
    "    dim3 gridDim((out_w + blockDim.x - 1) / blockDim.x, (out_h + blockDim.y - 1) / blockDim.y);\n",
    "\n",
    "    // Launch kernel\n",
    "    conv2d_k<<<gridDim, blockDim>>>(m, f, out, f_h / 2, m_h, m_w, f_h, f_w, out_h, out_w);\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def conv2d_const_k(m, out, r):\n",
    "    # get row and column indices\n",
    "    row,col = cuda.grid(2)\n",
    "    if row < out.shape[0] and col < out.shape[1]:  # Ensure threads are within output shape\n",
    "        val = 0\n",
    "        for i in range(f.shape[0]):\n",
    "            for j in range(f.shape[1]):\n",
    "                in_row = row - r + i\n",
    "                in_col = col - r +j\n",
    "                if (m.shape[0]>in_row >=0 and m.shape[1]>in_col >=0):\n",
    "                    val += m[in_row, in_col] * f[i, j]  # Convolution operation\n",
    "        out[row, col] = val  # Store result in output array\n",
    "\n",
    "\n",
    "def conv_const_2d(m, f):\n",
    "    h,w  = m.shape\n",
    "    out = torch.zeros(h, w, dtype=m.dtype, device=m.device)\n",
    "    # TOTAL block size is limited by 1024 threads\n",
    "    block_size = 32\n",
    "    f = cuda.const.array_like(filter)\n",
    "    blocks = cdiv(h,block_size), cdiv(w,block_size)\n",
    "    conv2d_const_k[blocks, (block_size, block_size)](ca(m), ca(out), f.shape[0]//2) \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "<function const.array_like at 0x7f6115b818a0> cannot be called from host code",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39misclose(conv(m1[\u001b[38;5;28;01mNone\u001b[39;00m,]), \u001b[43mconv_const_2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-7\u001b[39m)\u001b[38;5;241m.\u001b[39mall()\n",
      "Cell \u001b[0;32mIn[36], line 21\u001b[0m, in \u001b[0;36mconv_const_2d\u001b[0;34m(m, f)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# TOTAL block size is limited by 1024 threads\u001b[39;00m\n\u001b[1;32m     20\u001b[0m block_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[0;32m---> 21\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray_like\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m blocks \u001b[38;5;241m=\u001b[39m cdiv(h,block_size), cdiv(w,block_size)\n\u001b[1;32m     23\u001b[0m conv2d_const_k[blocks, (block_size, block_size)](ca(m), ca(out), f\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m) \n",
      "File \u001b[0;32m~/coding/cuda-pmpp/.conda/lib/python3.11/site-packages/numba/cuda/stubs.py:33\u001b[0m, in \u001b[0;36mstub_function.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m cannot be called from host code\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m fn)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: <function const.array_like at 0x7f6115b818a0> cannot be called from host code"
     ]
    }
   ],
   "source": [
    "torch.isclose(conv(m1[None,]), conv_const_2d(m1,f), atol=1e-7).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "621 µs ± 13.5 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit conv_const_2d(m1,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
